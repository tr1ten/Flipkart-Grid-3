{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Recommendation Algorithm\n",
    "### Existing User\n",
    "#### Collaborative Filtering: \n",
    "Save user actions such as purchasing history, browsing history and generate interaction matrix for collaborative filtering algorithm, then recommend items to users based on their similarity with other users.\n",
    "\n",
    "### New User\n",
    "#### Content-Based Filtering:\n",
    "Ask user for basic demographic information and recommend items based on their similarity with other items.\n",
    "Item features such as price, cluster (category) also included in the recommendation algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lightfm.data import Dataset\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import auc_score, precision_at_k, recall_at_k\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from lightfm import LightFM\n",
    "from sklearn.base import clone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accomodate new users, items into old model\n",
    "# credits: https://github.com/lyst/lightfm/issues/347#issuecomment-707829342\n",
    "class LightFMResizable(LightFM):\n",
    "    \"\"\"A LightFM that resizes the model to accomodate new users,\n",
    "    items, and features\"\"\"\n",
    "\n",
    "    def fit_partial(\n",
    "        self,\n",
    "        interactions,\n",
    "        user_features=None,\n",
    "        item_features=None,\n",
    "        sample_weight=None,\n",
    "        epochs=1,\n",
    "        num_threads=1,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        try:\n",
    "            self._check_initialized()\n",
    "            self._resize(interactions, user_features, item_features)\n",
    "        except ValueError:\n",
    "            # This is the first call so just fit without resizing\n",
    "            pass\n",
    "\n",
    "        super().fit_partial(\n",
    "            interactions,\n",
    "            user_features,\n",
    "            item_features,\n",
    "            sample_weight,\n",
    "            epochs,\n",
    "            num_threads,\n",
    "            verbose,\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _resize(self, interactions, user_features=None, item_features=None):\n",
    "        \"\"\"Resizes the model to accommodate new users/items/features\"\"\"\n",
    "\n",
    "        no_components = self.no_components\n",
    "        no_user_features, no_item_features = interactions.shape  # default\n",
    "\n",
    "        if hasattr(user_features, \"shape\"):\n",
    "            no_user_features = user_features.shape[-1]\n",
    "        if hasattr(item_features, \"shape\"):\n",
    "            no_item_features = item_features.shape[-1]\n",
    "\n",
    "        if (\n",
    "            no_user_features == self.user_embeddings.shape[0]\n",
    "            and no_item_features == self.item_embeddings.shape[0]\n",
    "        ):\n",
    "            return self\n",
    "\n",
    "        new_model = clone(self)\n",
    "        new_model._initialize(no_components, no_item_features, no_user_features)\n",
    "\n",
    "        # update all attributes from self._check_initialized\n",
    "        for attr in (\n",
    "            \"item_embeddings\",\n",
    "            \"item_embedding_gradients\",\n",
    "            \"item_embedding_momentum\",\n",
    "            \"item_biases\",\n",
    "            \"item_bias_gradients\",\n",
    "            \"item_bias_momentum\",\n",
    "            \"user_embeddings\",\n",
    "            \"user_embedding_gradients\",\n",
    "            \"user_embedding_momentum\",\n",
    "            \"user_biases\",\n",
    "            \"user_bias_gradients\",\n",
    "            \"user_bias_momentum\",\n",
    "        ):\n",
    "            # extend attribute matrices with new rows/cols from\n",
    "            # freshly initialized model with right shape\n",
    "            old_array = getattr(self, attr)\n",
    "            old_slice = [slice(None, i) for i in old_array.shape]\n",
    "            new_array = getattr(new_model, attr)\n",
    "            new_array[tuple(old_slice)] = old_array\n",
    "            setattr(self, attr, new_array)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProductTitle    object\n",
       "Image           object\n",
       "Price            int64\n",
       "cluster          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv',index_col=0)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_features():\n",
    "    user_feaures = ['sex:M','sex:F','sex:P']\n",
    "    for i in range(100): user_feaures.append(\"age:\"+str(i))\n",
    "    return user_feaures\n",
    "# dummy user for testing purposes\n",
    "users = [\n",
    "    {\n",
    "    'id': 1,\n",
    "    'age': 19,\n",
    "    'sex': 'F',\n",
    "    },\n",
    "     {\n",
    "    'id': 2,\n",
    "    'age': 40,\n",
    "    'sex': 'M',\n",
    "    },   \n",
    "]\n",
    "def get_item_features():\n",
    "    item_features = []\n",
    "    for i in df['cluster'].unique(): item_features.append(\"cluster:\"+str(i))\n",
    "    for p in df['Price'].unique(): item_features.append(\"price:\"+str(int(p)))\n",
    "    return item_features\n",
    "dummy_user_interaction = [(1,0,2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Dataset for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "user_features = get_user_features()\n",
    "item_features = get_item_features()\n",
    "dataset.fit([x['id'] for x in users],df.index,user_features=user_features,item_features=item_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 477893)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users, n_items = dataset.interactions_shape()\n",
    "n_users, n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_map, user_feature_map, item_id_map, item_feature_map = dataset.mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_interaction(userId:int,it=dict()):\n",
    "    ret = []\n",
    "    for x in df.index:\n",
    "        if x in it:\n",
    "            ret.append((userId,x,it[x]))\n",
    "        else: \n",
    "            ret.append((userId,x,0))\n",
    "    return ret\n",
    "def gen_user_feature(user:object):\n",
    "    feat = []\n",
    "    for x in user:\n",
    "        if x=='id': continue\n",
    "        feat.append(x + ':' + str(user[x]))\n",
    "    return (user['id'],feat)\n",
    "def gen_item_feature(id,item:object):\n",
    "    try:\n",
    "\n",
    "        return [id,['cluster:'+str(item['cluster']),'price:'+str(int(item['Price']))]]\n",
    "    except Exception as e:\n",
    "        print(\" erro in \",id,item)\n",
    "        raise(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "(interactions, weights) = dataset.build_interactions(dummy_user_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs = [ gen_user_feature(users[0]),gen_user_feature(users[1]) ]\n",
    "ufs = dataset.build_user_features( nfs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "for x in df.index:\n",
    "    feats.append(gen_item_feature(x,df.loc[x]))\n",
    "ifs = dataset.build_item_features(feats)\n",
    "# get row \n",
    "# [(x,['cluster:'+str(df.loc[x]['cluster'])]) for x in df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_map, user_feature_map, item_id_map, item_feature_map = dataset.mapping()\n",
    "id_item_map = dict(((x,y) for y,x in item_id_map.items()))\n",
    "# user_feature_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LightFMResizable at 0x7f3b0e980610>"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LightFMResizable(loss='warp')\n",
    "model.fit(interactions,user_features=ufs,item_features=ifs,sample_weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to add and update user model\n",
    "# add interaction data for old users\n",
    "LIMIT = 10000\n",
    "all_items = np.arange(LIMIT)\n",
    "def update_user(user_id, item_id, count):\n",
    "    (new_interactions,new_wts) = dataset.build_interactions([(user_id, item_id, count)])\n",
    "    ufs = dataset.build_user_features(nfs)\n",
    "    model.fit_partial(new_interactions,user_features=ufs,item_features=ifs, sample_weight=new_wts)\n",
    "# add interaction data for new users\n",
    "def add_user(user):\n",
    "    global nfs\n",
    "    dataset.fit_partial(users=[user['id']])\n",
    "    nfs += ([ gen_user_feature(user) ])\n",
    "def get_item(id):\n",
    "    return df.loc[id]\n",
    "# get recommendation for existing user\n",
    "def recommend(userId):\n",
    "    y = model.predict(userId,all_items)\n",
    "    return [get_item(id_item_map[x]) for x in np.argsort(-y)] [:4]\n",
    "    # return np.argsort(-y)[:4]\n",
    "# get recommendation for new user who has not interacted with any item\n",
    "def format_newuser_input(user_feature_map, user_feature_list):\n",
    "  normalised_val = 1.0 \n",
    "  target_indices = []\n",
    "  for feature in user_feature_list:\n",
    "    try:\n",
    "        target_indices.append(user_feature_map[feature])\n",
    "    except KeyError:\n",
    "        print(\"new user feature encountered '{}'\".format(feature))\n",
    "        pass\n",
    "\n",
    "  new_user_features = np.zeros(len(user_feature_map.keys()))\n",
    "  for i in target_indices:\n",
    "    new_user_features[i] = normalised_val\n",
    "  new_user_features = sparse.csr_matrix(new_user_features)\n",
    "  return(new_user_features)\n",
    "def new_user_recommend(user):\n",
    "    new_user_features = gen_user_feature(user)[-1]\n",
    "    new_ufs = format_newuser_input(user_feature_map, new_user_features)\n",
    "    y = model.predict(0,np.arange(n_items),user_features=new_ufs)\n",
    "    return [get_item(id_item_map[x]) for x in np.argsort(-y)] [:4]\n",
    "    # return  np.argsort(y)[:4]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Metric for Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate test and train based on the user type, then calculate the performance metric for recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "SIZE = 20\n",
    "def gen_interaction(userId,ids):\n",
    "    ret = []\n",
    "    for id in ids:\n",
    "        ret.append((userId,id,1))\n",
    "    return ret\n",
    "def gen_user(start,end,age,gender):\n",
    "    for i in range(start,end+1):\n",
    "        add_user({'id':i,'age':age,'sex':gender})\n",
    "    return list(range(start,end+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_train_terms = ['boys shoes', 'helmet', 'watches for men', 'cool hoodies for boys']\n",
    "male_test_terms = ['men\\'s  dress', 'men\\'s shorts', 'men\\'s sunglasses',' classic watches ']\n",
    "\n",
    "female_train_terms = ['women\\'s dresses', 'high heels', 'handbags', 'makeup']\n",
    "female_test_terms = ['women\\'s winter coats', 'women\\'s heels','girls top','handbags']\n",
    "# based on above we create item ids using our search based algorithm\n",
    "male_train_ids = [173357,333098,380969,451515,451675,452551,460693,468030,1738,2483,394585,10312,10324,10343,10347,10355,10426,10432,10448,10452,377213,521045,521107,17604,26679,27524,58641,333165,336905,336952,236116,329796,551034,551047,23547,29191,331641,393513,550825,28797]\n",
    "male_test_ids =  [117825,138282,147046,532784,533296,533545,540792,542829,543907,547486,113440,368557,544612,546121,546208,116440,532522,532744,533046,533168,105169,109310,112581,362566,379966,416600,417115,423692,425726,430281,27524,42476,58641,333486,333557,333794,333829,336661,336783,336906]\n",
    "\n",
    "female_train_ids = [109973,265312,416392,530618,534157,544407,547492,539016,541736,545724,4138,9336,9403,24500,49995,126825,192874,192898,193222,193689,27584,38679,41065,45072,46754,49846,50199,50505,51642,51848,26757,58663,371812,371900,371968,372322,372371,372735,372753,372780]\n",
    "female_test_ids = [109973,265312,416392,530618,534157,544407,547492,539016,541736,545724,4138,9336,9403,24500,49995,126825,192874,192898,193222,193689,27584,38679,41065,45072,46754,49846,50199,50505,51642,51848,26757,58663,371812,371900,371968,372322,372371,372735,372753,372780]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding users\n",
    "AGE = 20\n",
    "train_cap = 5000 # number of user in test and train data \n",
    "m_train_uids = gen_user(10,10+train_cap,AGE,'M')\n",
    "f_train_uids = gen_user(10+2*train_cap,10+3*train_cap,AGE,'F')\n",
    "test_cap = 2000\n",
    "f_test_uids = gen_user(10+3*train_cap,10+3*train_cap + test_cap,AGE,'F')\n",
    "m_test_uids = gen_user(10+3*train_cap + test_cap,10+3*train_cap + 2*test_cap,AGE,'M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_it = []\n",
    "for g in ['M','F']:\n",
    "    for uid in (m_train_uids if g=='M' else f_train_uids):\n",
    "        train_it.extend(gen_interaction(uid,male_train_ids  if g=='M' else female_train_ids))\n",
    "test_it = []\n",
    "for g in ['M','F']:\n",
    "    for uid in (m_test_uids if g=='M' else f_test_uids):\n",
    "        test_it.extend(gen_interaction(uid,male_test_ids  if g=='M' else female_test_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_interaction,train_wts = dataset.build_interactions(train_it)\n",
    "test_interaction,_ = dataset.build_interactions(test_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4003, 4106)"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufs = dataset.build_user_features(nfs)\n",
    "ufs.todense().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model performance based on the following metrics:\n",
    "- ROC AUC : Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "- Precision@k : Precision at k (P@k) metric computes the percentage of recommended items in the top-k list that are relevant to the user.\n",
    "- Recall@k : Recall at k (R@k) metric computes the percentage of relevant items that are recommended in the top-k list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC: 1.00\n",
      "Train AUC: 0.81\n"
     ]
    }
   ],
   "source": [
    "# auc\n",
    "model.fit(train_interaction,user_features=ufs,item_features=ifs,sample_weight=train_wts,epochs=50)\n",
    "train_auc = auc_score(model,train_interaction,user_features=ufs,item_features=ifs).mean()\n",
    "test_auc = auc_score(model,test_interaction,user_features=ufs,item_features=ifs).mean()\n",
    "print(\"Train AUC: %.2f\" % (train_auc))\n",
    "print(\"Train AUC: %.2f\" % (test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Precision @k  0.9891109\n",
      "Test Precision @k  0.59995\n"
     ]
    }
   ],
   "source": [
    "#precision@k\n",
    "train_precision = precision_at_k(model,train_interaction,user_features=ufs,item_features=ifs).mean()\n",
    "test_precision = precision_at_k(model,test_interaction,user_features=ufs,item_features=ifs).mean()\n",
    "print(\"Train Precision @k \" ,(train_precision));\n",
    "print(\"Test Precision @k \" ,(test_precision));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recall@k\n",
    "train_recall = recall_at_k(model,train_interaction,user_features=ufs,item_features=ifs).mean()\n",
    "test_recall = recall_at_k(model,test_interaction,user_features=ufs,item_features=ifs).mean()\n",
    "print(\"Train recall @k %.2f\" % (train_recall));\n",
    "print(\"Test recall @k \",(test_recall));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why so low metrics?\n",
    "Mainly duet to test data which generated based on fixed parameters which might not do justice for the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misc\n",
    "save data for server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# pcickle both the model and the dataset\n",
    "# pickle.dump(model, open('model.pkl','wb'),protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# pickle.dump(dataset, open('dataset.pkl','wb'),protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# pickle.dump(nfs, open('nfs.pkl','wb'),protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# pickle.dump(ifs, open('ifs.pkl','wb'),protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# pickle.dump(user_feature_map, open('user_feature_map.pkl','wb'),protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# pickle.dump(id_item_map, open('id_item_map.pkl','wb'),protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
